#!/usr/bin/env python

import sys, os

# first determine the top level directory (Ie. /path/to/ssscrape)
topdir = os.path.normpath(os.path.join(os.path.abspath(sys.argv[0]), os.pardir, os.pardir))

# then add the lib/ and lib/ext/ paths to sys.path
sys.path.insert(0, os.path.join(topdir, 'lib'))
sys.path.insert(0, os.path.join(topdir, 'lib', 'ext'))

import optparse
import MySQLdb
import _mysql_exceptions

from twisted.internet import reactor
from twisted.python import log

import ssscrapeapi
import ssscrapeapi.feeds

if __name__ == '__main__':

    # Handle command line options

    parser = optparse.OptionParser(usage="%prog -f feed_id [-t task_id] [-p period]")
    parser.add_option(
            '-f', '--feed',
            dest='feed_id', metavar='FEEDID', default=None,
            help="The feed id for which to refetch empty permalinks.")
    parser.add_option(
            '-t', '--tag',
            dest='tag', metavar='TAG', default=None,
            help="The tag for which to refetch empty permalinks.")
    parser.add_option(
            '-p', '--period',
            dest='period', metavar='TIMESPEC', default='1d',
            help="Schedule time, defaults to the literal '1d'.")
    (options, args) = parser.parse_args()

    if not (options.feed_id or options.tag):
        parser.error('Please specify a feed id or a tag')

    # Parse schedule date

    import datetime

    try:
        period_seconds = ssscrapeapi.misc.parse_time_string_to_seconds(options.period)
    except ValueError:
        parser.error('Invalid time specification: %s' % options.period);

    # connect to the DB
    ssscrapeapi.database.connect()
    ssscrapeapi.database.connect('database')

    feeds = []
    if options.feed_id:
        feeds.append(options.feed_id)
    else:
        cursor = ssscrapeapi.database.execute('''SELECT feed_id FROM ssscrape_feed_metadata WHERE FIND_IN_SET(%s, tags)''', (options.tag,))
        feeds = [feed_id for [feed_id] in cursor.fetchall()]

    for feed in feeds:
        # load some meta information about the feed
        metadata = ssscrapeapi.feeds.FeedMetadata(feed_id=feed)
        metadata.find()

        metadata.load(metadata['id'])

        if not metadata.has_key('id'):
            continue

        # load information about the task
        if metadata.has_key('url') and metadata['url'] == '':
            continue

        cursor = ssscrapeapi.database.execute('''SELECT t.id FROM ssscrape_feed_metadata m LEFT JOIN ssscrapecontrol.ssscrape_task t ON LOCATE(m.url, t.args) WHERE m.id = %s''', (metadata['id'], ))

        if cursor: 
            task = ssscrapeapi.Task()
            row = cursor.fetchone()
            print row
            task.load(row[0])
        else:
            task = None

        print >>sys.stderr, metadata['feed_id'], task['id']

        # find all the empty articles in the timespan
        cursor = ssscrapeapi.database.execute('''SELECT id FROM ssscrape_feed_item WHERE feed_id = %s AND (content_clean IS NULL OR content_clean = '') AND pub_date >= (NOW() - INTERVAL %s SECOND) ORDER BY id''', (feed, period_seconds))
        for row in cursor.fetchall():
            # instantiate a new job with the right properties
            job = ssscrapeapi.Job()
            job['type'] = ssscrapeapi.config.get_string('feeds', 'default-partial-type')
            job['program'] = ssscrapeapi.config.get_string('feeds', 'default-partial-program')
            job['args'] =  "-i %s %s" % (row[0], metadata['partial_args'])

            if task:
                # if the task was specified, we can copy the hostname conditions
                job['hostname'] = task['hostname']
                job['resource_id'] = task['resource_id']

            # now try to find a job that looks like this one
            id = job.find()
        
            # if we did not find it....
            if id <= 0: 
                # then we can schedule the job
                job['scheduled'] = 'NOW()'
                job.unescaped = ['scheduled']
                job.save()

                print "Scheduled job for item %s" % (row[0]) 

    # disconnect from DB
    ssscrapeapi.database.disconnect('database')
    ssscrapeapi.database.disconnect()
